<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Machine Learning Code Examples</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
      }
      header {
        background-color: #333;
        color: white;
        padding: 10px 0;
        text-align: center;
      }
      main {
        padding: 20px;
      }
      .section {
        margin-bottom: 40px;
      }
      pre {
        background-color: #222;
        color: #fff;
        padding: 10px;
        border-radius: 5px;
        font-size: 14px;
        overflow-x: auto;
      }
      h2 {
        color: #333;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>Machine Learning Code Examples</h1>
    </header>

    <main>
      <!--  -->
      <div class="section">
        <h2>Start</h2>
        <pre>
            P1 FNN

            import numpy as np
            import pandas as pd
            import tensorflow as tf
                        
                        
            tf.__version__
                        
            dataset = pd.read_csv("C:/Users/HP/Downloads/Churn_Modelling.csv")
            X = dataset.iloc[:, 3:-1].values
            Y = dataset.iloc[:, -1].values
                        
            print(X)
                        
            print(Y)
                        
            
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            X[:, 2] = le.fit_transform(X[:, 2])
                        
            print(X)
                        
            from sklearn.compose import ColumnTransformer
            from sklearn.preprocessing import OneHotEncoder
            ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')
            X = np.array(ct.fit_transform(X))
                        
            print(X)
                        
            from sklearn.model_selection import train_test_split
            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)
                        
                        
            from sklearn.preprocessing import StandardScaler
            sc = StandardScaler()
            X_train = sc.fit_transform(X_train)
            X_test = sc.transform(X_test)
                        
                        
            ann = tf.keras.models.Sequential()
            ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
            ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
            ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
            ann.compile(optimizer = 'adadelta', loss = 'binary_crossentropy', metrics = ['accuracy'])
            ann.fit(X_train, Y_train, batch_size = 32, epochs = 100)
            
            Y_pred = ann.predict(X_test)
            Y_pred = (Y_pred > 0.5)
            print(np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1))
                        
            from sklearn.metrics import confusion_matrix, accuracy_score
            cm = confusion_matrix(Y_test, Y_pred)
            print(cm)
            accuracy_score(Y_test, Y_pred)
            
            
            --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
            
            
            P2 Regularization/Overfitting
            
            
            import numpy as np
            import matplotlib.pyplot as plt
            import pandas as pd
            from sklearn import metrics
            from sklearn.linear_model import Lasso
            from sklearn.linear_model import Ridge
            
            dataset = pd.read_csv("C:/Users/HP/Downloads/Salary_Data.csv")
            X = dataset.iloc[:, :-1].values
            y = dataset.iloc[:, -1].values
            
            print(X)
            print(y)
            
            #splitting the dataset into training and testingg set
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/3, random_state=0)
            
            print(X_train)
            
            print(X_test)
            
            print(y_train)
            
            print(y_test)
            
            lasso = Lasso()
            
            lasso.fit(X_train, y_train)
            
            print("Lasso Train RMSE:", np.round(np.sqrt(metrics.mean_squared_error(y_train, lasso.predict(X_train))), 5))
            
            print("Lasso Test RMSE:",np.round(np.sqrt(metrics.mean_squared_error(y_test, lasso.predict(X_test))), 5))
            
            ridge=Ridge()
            
            ridge.fit(x_train,y_train)
            
            print("Ridge Train RMSE:", np.round(np.sqrt(metrics.mean_squared_error(y_train, ridge.predict(x_train))),5))
            
            print("Ridge Test RMSE:", np.round(np.sqrt(metrics.mean_squared_error(y_test, ridge.predict(x_test))),5))
            
            
            --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
            
            P3 CNN
            
            import tensorflow as tf
            from tensorflow.keras.preprocessing.image import ImageDataGenerator
            
            tf.__version__
            
            train_datagen = ImageDataGenerator(rescale = 1./255,
                                               shear_range = 0.2,
                                               zoom_range = 0.2,
                                               horizontal_flip = True)
            training_set = train_datagen.flow_from_directory('C:/Users/HP/Downloads/small_dataset/training_set',
                                                             target_size = (64, 64),
                                                             batch_size = 32,
                                                             class_mode = 'binary')
            
            
            test_datagen = ImageDataGenerator(rescale = 1./255)
            test_set = test_datagen.flow_from_directory('C:/Users/HP/Downloads/small_dataset/test_set',
                                                        target_size = (64, 64),
                                                        batch_size = 32,
                                                        class_mode = 'binary')
            
            
            cnn = tf.keras.models.Sequential()
            
            cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))
            
            cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
            
            cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
            cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
            
            cnn.add(tf.keras.layers.Flatten())
            
            cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))
            
            cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
            
            cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
            
            cnn.fit(x = training_set, validation_data = test_set, epochs = 25 )
            
            import numpy as np
            from keras.preprocessing import image
            test_image = image.load_img('C:/Users/HP/Downloads/small_dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))
            test_image = image.img_to_array(test_image)
            test_image = np.expand_dims(test_image, axis = 0)
            result = cnn.predict(test_image)
            training_set.class_indices
            if result[0][0] == 1:
              prediction = 'dog'
            else:
              prediction = 'cat'
            
            print(prediction)
            
            
            --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
            
            P4 CIFAR-10
            
            
            import tensorflow as tf
            print(tf.__version__)
            import numpy as np 
            import matplotlib.pyplot as plt
            from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout
            from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D
            from tensorflow.keras.layers import BatchNormalization
            from tensorflow.keras.models import Model
            
            
            #load in the data
            cifar10=tf.keras.datasets.cifar10
            
            #distribute it to train and test set
            (x_train, y_train), (x_test, y_test)=cifar10.load_data()
            print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)
            
            #reduce pixel values
            x_train ,x_test=x_train/255.0, x_test/255.0
            
            #flatten the label values
            y_train, y_test= y_train.flatten(), y_test.flatten()
            
            #visualize data by plotting imgs
            fig,ax=plt.subplots(5,5)
            k=0
            
            for i in range(5):
                for j in range(5):
                    ax[i][j].imshow(x_train[k], aspect='auto')
                    k+=1
                    
            plt.show()
            
            
            #build CNN model
            
            #no. of classes
            K=len(set(y_train))
            
            # calculate total no of classes
            #for output layer
            print("no. of classes:", K)
            
            #build model using functional api
            #input layer
            i=Input(shape=x_train[0].shape)
            x=Conv2D(32, (3,3), activation='relu', padding='same')(i)
            x=BatchNormalization()(x)
            x=Conv2D(32, (3,3), activation='relu', padding='same')(x)
            x=BatchNormalization()(x)
            x=MaxPooling2D((2,2))(x)
            
            x=Conv2D(64,(3,3), activation='relu', padding='same')(x)
            x=BatchNormalization()(x)
            x=Conv2D(64,(3,3), activation='relu', padding='same')(x)
            x=BatchNormalization()(x)
            x=MaxPooling2D((2,2))(x)
            
            x=Conv2D(128,(3,3), activation='relu', padding='same')(x)
            x=BatchNormalization()(x)
            x=Conv2D(128,(3,3), activation='relu', padding='same')(x)
            x=BatchNormalization()(x)
            x=MaxPooling2D((2,2))(x)
            
            x=Flatten()(x)
            x=Dropout(0.2)(x)
            
            #hidden layer
            x=Dense(1024, activation='relu')(x)
            x=Dropout(0.2)(x)
            
            #last hidden layer i.e. output layer
            x=Dense(K, activation='softmax')(x)
            
            model=Model(i,x)
            
            #model ddescription
            model.summary()
            
            
            #compile the cnn model
            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            
            #fit the model
            r=model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2)
            
            #fit the data augmentation
            batch_size=32
            data_generator=tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)
            
            train_generator=data_generator.flow(x_train, y_train, batch_size)
            steps_per_epoch=x_train.shape[0]//batch_size
            
            r=model.fit(train_generator, validation_data=(x_test,y_test), steps_per_epoch=steps_per_epoch, epochs=2)
            
            
            #plot accuracy per iteration
            plt.plot(r.history['accuracy'],label='acc',color='red')
            plt.plot(r.history['val_accuracy'],label='val_acc',color='green')
            plt.legend()
            
            #label mapping
            labels= '''airplane automobile bird cat deer dog frog horse ship truck'''.split()
            #select img from dataset
            image_number=0
            #display img
            plt.imshow(x_test[image_number])
            #load img in array
            n=np.array(x_test[image_number])
            #reshape it
            p=n.reshape(1,32,32,3)
            #pass in n/w for prediction and
            #save predicted label
            predicted_label=labels[model.predict(p).argmax()]
            #load original label
            original_label=labels[y_test[image_number]]
            #display result
            print("Original label is {} and predicted label is {}".format(original_label, predicted_label))
            
            
            --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
            
            
            P5 Autoencoder
            
            from keras.layers import Input, Dense
            from keras.models import Model
            from keras.datasets import mnist
            import numpy as np
            import matplotlib.pyplot as plt
            %matplotlib inline
            
            #load our mnist dataset
            (XTrain, YTrain), (XTest, YTest) = mnist.load_data()
            
            print('XTrain class = ', type(XTrain))
            print('YTrain class = ', type(YTrain))
            
            #shape of our dataset
            print('XTrain shape = ', XTrain.shape)
            print('XTrain shape = ', XTest.shape)
            print('YTrain shape = ', YTrain.shape)
            print('YTest shape = ', YTest.shape)
            
            #number of distinct values of our mnist target
            print('YTrain values = ', np.unique(YTrain))
            print('YTest values = ', np.unique(YTest))
            
            #distribution of classes in our dataset
            unique, counts = np.unique(YTrain, return_counts=True)
            print('YTrain distribution = ', dict(zip(unique, counts)))
            unique, counts = np.unique(YTest, return_counts=True)
            print('YTest distribution = ', dict(zip(unique, counts)))
            
            
            #we plot an histogram distribution of our test and train data
            fig, axs = plt.subplots(1, 2, figsize=(15, 5))
            axs[0].hist(YTrain, ec = 'black')
            axs[0].set_title('YTrain data')
            axs[0].set_xlabel('Classes')
            axs[0].set_ylabel('Number of occurences')
            axs[1].hist(YTest, ec = 'black')
            axs[1].set_title('YTest data')
            axs[1].set_xlabel('Classes')
            axs[1].set_ylabel('Number of occurences')
            #we want to show all ticks...
            axs[0].set_xticks(np.arange(10))
            axs[1].set_xticks(np.arange(10))
            plt.show()
            
            
            from re import X
            #data normalization
            XTrain = XTrain.astype('float32')/255
            XTest = XTest.astype('float32')/25
            
            #data reshaping
            XTrain = XTrain.reshape((len(XTrain), np.prod(XTrain.shape[1:])))
            XTest = XTest.reshape((len(XTest), np.prod(XTest.shape[1:])))
            
            print(XTrain.shape)
            print(XTest.shape)
            
            
            InputModel = Input(shape=(784,))
            EncodedLayer = Dense(32, activation='relu')(InputModel)
            DecodedLayer = Dense(784, activation='sigmoid')(EncodedLayer)
            AutoencoderModel = Model(InputModel, DecodedLayer)
            #we can sumarize our model
            AutoencoderModel.summary()
            
            
            #lets train the model using adadelta optimizer
            #autoencodermodel.compile(optimizer = 'adadelta', loss = 'binary_crossentropy) or
            AutoencoderModel.compile(optimizer = 'adam', loss = 'binary_crossentropy')
            history = AutoencoderModel.fit(XTrain, XTrain, epochs = 100, batch_size = 256, shuffle = True, validation_data = (XTest, XTest))
            #make prediction to decode the digits
            DecodedDigits = AutoencoderModel.predict(XTest)
            
            
            def plotmodelhistory(history):
                plt.plot(history.history['loss'])
                plt.plot(history.history['val_loss'])
                plt.title('Autoencoder Model loss')
                plt.ylabel('Loss')
                plt.xlabel('Epoch')
                plt.legend(['Train', 'Test'], loc = 'upper left')
                plt.show()
            
            #list all data in history
            print(history.history.keys())
            #visualization of the loss minimization during the training process
            plotmodelhistory(history)
            
            
            n = 5
            plt.figure(figsize = (20, 4))
            for i in range(n):
                ax = plt.subplot(2, n, i+1)
                #input image
                plt.imshow(XTest[i+10].reshape(28, 28))
                plt.gray
                ax.get_xaxis().set_visible(False)
                ax.get_yaxis().set_visible(False)
                ax = plt.subplot(2, n, i+1+n)
                #image decoded by our auto encoder
                plt.imshow(DecodedDigits[i+10].reshape(28, 28))
                plt.gray()
                ax.get_xaxis().set_visible(False)
                ax.get_yaxis().set_visible(False)
            plt.show()
            
            
            --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
            
            P6 Digit recognition
            
            
            import numpy as np
            import keras
            from keras.datasets import mnist
            from keras.models import Model
            from keras.models import Sequential
            from keras.layers import Dense, Dropout, Flatten
            from keras.layers import Conv2D, MaxPooling2D, Input
            from keras import backend as K
            import matplotlib.pyplot as plt
            
            (x_train, y_train), (x_test, y_test) = mnist.load_data()
            
            
            img_rows, img_cols = 28, 28
            if K.image_data_format() == 'channels_first':
                x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
                x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
                input_shape = (1, img_rows, img_cols)
            else:
                x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
                x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
                input_shape = (img_rows, img_cols, 1)
            x_train = x_train.astype('float32')
            x_test = x_test.astype('float32')
            x_train /= 255
            x_test /= 255
            
            
            y_train = keras.utils.to_categorical(y_train)
            y_test = keras.utils.to_categorical(y_test)
            
            
            inpx= Input(shape = input_shape)
            layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx)
            layer2 = Conv2D(64, (3, 3), activation='relu')(layer1)
            layer3 = MaxPooling2D(pool_size=(2, 2))(layer2)
            layer4 = Dropout(0.25)(layer3)
            layer5 = Flatten()(layer4)
            layer6 = Dense(256, activation='sigmoid')(layer5)
            layer7 = Dense(10, activation = 'softmax')(layer6)
            
            model = Model([inpx], layer7)
            model.compile(optimizer=keras.optimizers.Adadelta(),loss=keras.losses.categorical_crossentropy,metrics=['accuracy'])
            model.fit(x_train, y_train,epochs = 12, batch_size=500)
            
            score = model.evaluate(x_test, y_test, verbose=0)
            print('Test loss:', score[0])
            print('Test accuracy:', score[1])
            
            y_predicted_by_model = model.predict(x_test)
            
            y_predicted_by_model[0]
            
            
            np.argmax(y_predicted_by_model[6])
            
            np.argmax(y_test[6])
            
            
            plt.imshow(x_test[6])
            
            
            ------------------------------------------------------------------------------------------------------------------------------------------------------------
            ------------------------------------------------------------------------------------------------------------------------------------------------------------
            P9 RNN 
            
            from tensorflow.keras.layers import SimpleRNN, LSTM, GRU,Bidirectional,Dense,Embedding
            from tensorflow.keras.datasets import imdb
            from tensorflow.keras.models import Sequential
            import numpy as np
            
            
            vocab_size = 5000
            (X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=vocab_size)
            print(X_train[0])
            
            word_idx = imdb.get_word_index()
            
            word_idx = {i:word for word, i in word_idx.items()}
            print([word_idx[i] for i in X_train[0]])
            
            print("Max length of a review: ",len(max((X_train + X_test), key=len)))
            print("Min length of a review: ",len(min((X_train + X_test),key=len)))
            
            from tensorflow.keras.preprocessing import sequence
            
            max_words = 400
            X_train = sequence.pad_sequences(X_train,maxlen = max_words)
            X_test = sequence.pad_sequences(X_test,maxlen = max_words)
            
            x_valid, y_valid = X_train[:64],y_train[:64]
            X_train_,y_train_ = X_train[64:],y_train[64:]
            
            
            embed_len = 32
            
            RNN_model = Sequential(name="Simple_RNN")
            RNN_model.add(Embedding(vocab_size,
                                    embd_len,
                                    input_length=max_words))
            
            # In case of a stacked(more than one layer of RNN)
            # use return_sequences=True
            RNN_model.add(SimpleRNN(128,
                                    activation='tanh',
                                    return_sequences=False))
            RNN_model.add(Dense(1, activation='sigmoid'))
            
            # printing model summary
            print(RNN_model.summary())
            
            # Compiling model
            RNN_model.compile(
                loss="binary_crossentropy",
                optimizer='adam',
                metrics=['accuracy']
            )
            
            # Training the model
            history = RNN_model.fit(x_train_, y_train_,
                                    batch_size=64,
                                    epochs=5,
                                    verbose=1,
                                    validation_data=(x_valid, y_valid))
            
            # Printing model score on test data
            print()
            print("Simple_RNN Score---> ", RNN_model.evaluate(x_test, y_test, verbose=0))
            
            
            
            # Defining GRU model
            gru_model = Sequential(name="GRU_Model")
            gru_model.add(Embedding(vocab_size,
                                    embd_len,
                                    input_length=max_words))
            gru_model.add(GRU(128,
                              activation='tanh',
                              return_sequences=False))
            gru_model.add(Dense(1, activation='sigmoid'))
            
            # Printing the Summary
            print(gru_model.summary())
            
            # Compiling the model
            gru_model.compile(
                loss="binary_crossentropy",
                optimizer='adam',
                metrics=['accuracy']
            )
            
            # Training the GRU model
            history2 = gru_model.fit(x_train_, y_train_,
                                     batch_size=64,
                                     epochs=5,
                                     verbose=1,
                                     validation_data=(x_valid, y_valid))
            
            # Printing model score on test data
            print()
            print("GRU model Score---> ", gru_model.evaluate(x_test, y_test, verbose=0))
            
            
            
            # Defining LSTM model
            lstm_model = Sequential(name="LSTM_Model")
            lstm_model.add(Embedding(vocab_size,
                                     embd_len,
                                     input_length=max_words))
            lstm_model.add(LSTM(128,
                                activation='relu',
                                return_sequences=False))
            lstm_model.add(Dense(1, activation='sigmoid'))
            
            # Printing Model Summary
            print(lstm_model.summary())
            
            # Compiling the model
            lstm_model.compile(
                loss="binary_crossentropy",
                optimizer='adam',
                metrics=['accuracy']
            )
            
            # Training the model
            history3 = lstm_model.fit(x_train_, y_train_,
                                      batch_size=64,
                                      epochs=5,
                                      verbose=2,
                                      validation_data=(x_valid, y_valid))
            
            # Displaying the model accuracy on test data
            print()
            print("LSTM model Score---> ", lstm_model.evaluate(x_test, y_test, verbose=0))
            
            
            
            --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
            
            P10 obj detection
            
            import cv2
            import matplotlib.pyplot as plt
            
            config_file = "C:/Users/HP/Downloads/ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt"
            frozen_model = "C:/Users/HP/Downloads/frozen_inference_graph.pb"
            
            model=cv2.dnn_DetectionModel(frozen_model,config_file)
            
            classLabels=[]
            file_name="C:/Users/HP/Downloads/labels.txt"
            with open(file_name,'rt') as fpt:
              classLabels=fpt.read().rstrip('\n').split('\n')
            
            
            print(classLabels)
            
            print(len(classLabels))
            
            model.setInputSize(320,320)
            model.setInputScale(1.0/127.5)
            model.setInputMean((127.5,127,5,127.5))
            model.setInputSwapRB(True)
            
            img = cv2.imread('C:/Users/HP/Downloads/dog.jpeg')
            plt.imshow(img)
            
            ClassIndex,Confidece,bbox=model.detect(img, confThreshold=0.5)
            
            print(ClassIndex)
            
            font_scale=3
            font=cv2.FONT_HERSHEY_PLAIN
            for ClassInd,conf,boxes in zip(ClassIndex.flatten(),Confidece.flatten(),bbox):
              cv2.rectangle(img,boxes,(255,0,0),2)
              cv2.putText(img,classLabels[ClassInd-1],(boxes[0]+10,boxes[1]+10),font,fontScale=font_scale,color=(0))
            
            
            plt.imshow(img)
            
            
            
            #for video
            
            cap=cv2.VideoCapture('Pip _ A Short Animated Film by Southeastern Guide Dogs.mp4')
            if not cap.isOpened():
              cap=cv2.VideoCapture(0)
            if not cap.isOpened():
              raise IOError('Cannot open the video')
            
            font_scale=3
            font=cv2.FONT_HERSHEY_PLAIN
            
            while True:
              ret, frame = cap.read()
              ClassIndex,confidece,bbox=model.detect(frame,confThreshold=0.55)
            
              print(ClassIndex)
            
              if(len(ClassIndex)!=0):
                for ClassInd, conf, boxes in zip(ClassIndex.flatten(), confidece.flatten(),bbox):
                  if(ClassInd<=80):
                    cv2.rectangle(img,boxes,(255,0,0),2)
                    cv2.putText(img,classLabels[ClassInd-1],(boxes[0]+10,boxes[1]+10),font,fontScale=font_scale,color=(0))
            
              cv2.imshow('object detection', frame)
            
              if cv2.waitKey(2) & 0xff == ord('q'):
                break
            
            cap.release()
            cv2.destroyaLLWindows()
            
            
            

        </pre>
      </div>
    </main>
  </body>
</html>
